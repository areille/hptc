\documentclass{article}
    
    \usepackage{hyperref}
    \usepackage{blkarray} % customed arrays
    \usepackage{amsmath} % matrix equations
    \usepackage{graphicx} % import images
    \usepackage{float} % floating imgs
    \usepackage{subcaption} % several figures on one line
    \usepackage{listings} %source code listing
    \lstset{
        basicstyle=\ttfamily\tiny
    }
    \graphicspath{ {ressources/} }
    
    \title{High Performance Technical Computing}
    \author{Augustin Reille}
    \date{2 November 2017}
    
    \begin{document}
        % Title
        \maketitle
    
        % Abstract
        \begin{abstract}
            A one-space dimensional problem is considered, to examine the application
            of distributed memory parallel programming techniques for the numerical 
            solution of the heat conduction equation.\\
            The effect of changing time and space steps are investigated, and so is the runtime
            between serial and parallel programs using MPI, resolving the same problem.
        \end{abstract}
    
        % Table of contents
        \newpage
        \tableofcontents
    
        % List of figures
        \newpage
        \listoffigures
    
        % List of tables
        \listoftables
    
        % List of listings
        \lstlistoflistings
    
        % List of abbreviations / Nomenclature
        \section*{Nomenclature}
            $T_i^n$ . . . Temperature at space $i$ and time $n$\\
            $D$ . . . . Diffusivity of the wall\\
            $L$ . . . . Total length of the wall\\
            $T$ . . . . Total time of the study\\
            $\Delta x$ . . . Space step\\
            $\Delta t$ . . . Time step\\
            $\delta x$ . . . . partial derivative according to space\\
            $\delta t$ . . . . partial derivative according to time\\
            $n_{space}$. . .  number of iterations over space\\
            $n_{pes}$. . . . number of processors\\
        % Introduction
        \newpage
        \section{Introduction}
    
        A PDE (Partial Differential Equation) is an equation which includes derivatives of an unknown function, with at least 2 variables.
        PDEs can describe the behavior of lot of engineering phenomena \cite{pde}, that's why it is important to 
        known how to resolve them computationally. One way to resolve PDEs is to discretize the problem into
        a mesh. The bigger is the mesh, the more accurate the results are.\\
        As runtime depends on the study size, a good way to provide a good accuracy for results
        and keeping a correct runtime is to parallelize the programs and the algorithms used.\\
        In this study, we will use MPI \cite{mpi_doc}, a well-known communication protocol for programming in parallel, to
        see how the accuracy and the runtime are evolving between a serial and a parallel implementation
        of a program resolving a one-dimensional PDE.
        
    
    
        % Methods / Procedures
        \newpage
        \section{Methods}
            Our study results are to be stored in matrixes.
            Several schemes are used :
            \begin{itemize}
                \item{FTCS simple explicit (forward time, central space, explicit)}
                \item{Laasonen simple implicit (forward time, central space, implicit)}
                \item{Crank-Nicholson (Trapezoidal)}
            \end{itemize}
            For all of the used schemes, the initialization of the result matrix is run the same way.
            First the study grid must be chosen. The initials conditions are:
            \begin{itemize}
                \item{$T_{int}$ of 100$^{\circ}$F}
                \item{$T_{sur}$ of 300$^{\circ}$F}
                \item{$D = 0.1 ft^{2}/hr$}
            \end{itemize}
            For each methods two sets of step size are to be used:
            \begin{enumerate}
                \item{$\Delta x = 0.5$ and $\Delta t = 0.1$} 
                \item{$\Delta x = 0.005$ and $\Delta t = 0.001$} 
            \end{enumerate}
            so that the result matrix could be initialized.
            We set:
            \begin{equation}
                n_{space} = \frac{L}{\Delta x} + 1
            \end{equation}
            \begin{equation}
                n_{time} = \frac{T}{\Delta t}
            \end{equation}
            $n_{space}$ is the number of iterations over the grid, according to the coordinate $x$. \\
            $n_{time}$ is the number of iterations over the grid, according to the time $t$. \\
            
            \[
                \begin{blockarray}{cccccc}
                i=0 & i=1 & \cdots & n_{space}-1 & n_{space} \\
                \begin{block}{(ccccc)c}
                  T_{ext} & T_{int} & \cdots & T_{int} & T_{ext} & j=0 \\
                  T_{ext} & 0 & \cdots & 0 & T_{ext} & j=1 \\
                  \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
                  T_{ext} & 0 & \cdots & 0 & T_{ext} & n_{time} \\
                \end{block}
                \end{blockarray}
            \]
            This is how the matrix is initialized, according to initial conditions.

            \subsection{FTCS simple explicit}
            
            In order to perform the parallelization, we assume that the number of 
            space steps can be divided by the number of processors. The number of
            space steps divided by the number of processors defines the new space
            step for each processor:
            \begin{equation}
                n'_{space} = \frac{n_{space}}{n_{pes}}
            \end{equation}
            We have the equation :
            \begin{equation}
                T_{i}^{n} = T_{i}^{n-1} + r(T_{i+1}^{n-1} - 2T_{i}^{n-1} + T_{i-1}^{n-1})
            \end{equation}
            with $$r = \frac{D\delta t}{\delta x^{2}}$$.
            The stencil for FTCS explicit scheme is the following:
            \begin{figure}[H]
                \includegraphics[width=4cm]{stencil_explicit.png}
                \centering
                \caption{Stencil for FTCS explicit scheme}
            \end{figure}

            That means that to calculate a temperature for a time step and a space
            step, we need some previous values in time : the value in space, 
            which we can access easily, and the previous and next values in space.
            That was the main problem for parallelizing this problem. The way we managed
            it is the following.\\
            For each processor, a vector of size $[n'_{space} + 2]$ is created. The size is incremented
            by 2 because each vector will contain 2 "ghost" values (will be explained later).\\
            The first vector will be our boundary values vector, so we fill this vector
            with $T_{int}$.\\
            For the first and last processor respectively, the first and last value of the vector are
            replaced by $T_{ext}$.\\
            For example, for $n_{space} = 12$ and $n_{pes} = 4$, at $t=0h$ we have :
            \begin{figure}[H]
                \includegraphics[width=\textwidth]{separation.png}
                \caption{Separation of values per processors at t=0h}
            \end{figure}

            At each time step, each processor perform 4 MPI actions:
            \begin{itemize}
                \item{it sends its first value to the previous processor (MPI\_Send()) }
                \item{it receives the first value of the next processor (MPI\_Receive())}
                \item{it sends its last value to the next processor (MPI\_Send())}
                \item{it receives the first value of the previous processor (MPI\_Receive())}
            \end{itemize}
            N-B : In the previous explanation, ghost values are not considered : they are only used
            to store the values received from neighbour processors.\\
            In Figure 3, we can see which values are exchanged at the boundaries, in order to
            calculate the next boundary value. This pattern is due to the stencil : to continue
            with the example of the Figure 3, in order to calculate the value of $h[1]$ at time step
            $t+\Delta{t}$, the processor needs the $h[0]$ value, which comes from the previous processor.
            \begin{figure}[H]
                \includegraphics[width=\textwidth]{parallel_explicit.png}
                \caption{Exchanges between processors at each time step}
            \end{figure}

            Calculated values are gathered (MPI\_Gather()) in the final result matrix, which is stored 
            and printed in root processor.

    
            \subsection{Implicit schemes}
    
                    For both implicit schemes, the vector solution is given by a matrix equation. 
                    Thomas algorithm is used to resolve the system for each space step.
    
                    \subsubsection{Laasonen}
    
                    We have :
                    \begin{equation}
                        \frac{T_{j}^{n+1} - T_{j}^n}{\Delta t} = D \frac{T_{j+1}^{n+1}- 2T_{j}^{n+1} + T_{j-1}^{n+1}}{\Delta x^2}
                    \end{equation}
                    The stencil for Laasonen implicit scheme is the following:
                    \begin{figure}[H]
                        \includegraphics[width=4cm]{stencil_implicit.png}
                        \centering
                        \caption{Stencil for Laasonen implicit scheme}
                    \end{figure}
    
                    Written in a matrical way, the solution at space step $n+1$ is given by :
                    \begin{equation}
                        \label{eq:laas}
                        \begin{bmatrix}
                            1+2C   & -C     & 0      & \cdots & 0 \\
                            -C     & 1+2C   & -C     & \ddots & \vdots \\
                            0      & -C     & \ddots & \ddots & 0 \\
                            \vdots & \ddots & \ddots & 1+2C   & -C\\
                            0      & \cdots & 0      & -C     & 1+2C
                        \end{bmatrix}
                        \begin{bmatrix}
                            T_{1} \\
                            T_{2} \\
                            \vdots \\
                            T_{i} \\
                            \vdots \\
                            T_{ntime}
                        \end{bmatrix}_{n+1}
                        =
                        \begin{bmatrix}
                            T_{1} + CT_{0}\\
                            T_{2} \\
                            \vdots \\
                            T_{i} \\
                            \vdots \\
                            T_{ntime} + CT_{f}
                        \end{bmatrix}_{n}
                    \end{equation}
                    with $$C = \frac{D\Delta t}{\Delta x ^2}  $$ 
                    The first vector at space $n$ is our boundaries conditions vector, so it is known.
                    In this scheme we must be aware that the right-term vector is actually bigger than
                    the other, of 2 values, which corresponds to the exterior temperatures, which stays
                    the same all the time.\\
                    In serial, we apply Thomas algorithm for each space step, at the reduced vector, and add the $n+1$ vector to our 
                    result matrix. The new right term vector is the one found with the Thomas algorithm.\\
                    There was several ways to parallelize this study. The one I chose is to
                    parallelize Thomas algorithm. The algorithm I used is described in \cite{thomasalg1}
                    and in \cite{thomasalg2}. The main idea is to make a recursive procedure with a LU decomposition.
                    We must note that this algorithm will work with a matrix with differents tridiagonal values, and only for
                    a number of processors which is a power of two.

                    \begin{equation}
                        \label{eq:laaslu}
                        \begin{bmatrix}
                            a_{1}  & c_{1}  & 0      & \cdots & 0 \\
                            b_{2}  & a_{2}  & c_{2}  & \ddots & \vdots \\
                            0      & b_{3}  & \ddots & \ddots & 0 \\
                            \vdots & \ddots & \ddots & a_{N-1}& c_{N-1}\\
                            0      & \cdots & 0      & b_{N}  & a_{N}
                        \end{bmatrix}
                        =
                        \begin{bmatrix}
                            1      &        &        &        &  \\
                             l_{2} & 1      &        &   0    &  \\
                                   &  l_{3} & \ddots &        & \\
                                   & 0      & \ddots & 1      & \\
                                   &        &        & l_{N}  & 1
                        \end{bmatrix}
                        \begin{bmatrix}
                            d_{1}  &  u_{1} &        &        &  \\
                                   & d_{2}  & u_{2}  &  0     &  \\
                                   &        & d_{3}  & u_{3} & \\
                                   & 0      &        & \ddots  & \ddots\\
                                   &        &        &        & d_{N}
                        \end{bmatrix}
                    \end{equation}
                    Then we have the relations between coefficients :
                    \begin{equation}
                        a_{1} = d_{1}
                    \end{equation}
                    \begin{equation}
                        \label{eq:cu}
                        c_{j} = u_{k}
                    \end{equation}
                    \begin{equation}
                        \label{eq:adlu}
                        a_{k} = d_{k} + l_{k}u_{k-1}
                    \end{equation}
                    \begin{equation}
                        \label{eq:bld}
                        b_{k} = l_{k}d_{k-1}
                    \end{equation}
                    where $j = 1,..., N$ and $k=2,...,N$.
                    By putting \ref{eq:cu} and \ref{eq:bld} into \ref{eq:adlu}, we have the recursive
                    relationship to find $d_{j}$:
                    \begin{equation}
                        d_{j} = \frac{a_{j}d_{j-1}-b_{j}c_{j-1}}{d_{j-1}}
                    \end{equation}
                    Then we create 2x2 matrices in order to process the parallelisation recursively (for $j=1,...,N$):
                    \begin{equation}
                        R_{0}=
                        \begin{bmatrix}
                            a_{0} & 0 \\
                            1     & 0
                        \end{bmatrix}
                    \end{equation}
                    and
                    \begin{equation}
                        R_{j}=
                        \begin{bmatrix}
                            a_{j} & -b_{j}c_{j-1} \\
                            1     & 0
                        \end{bmatrix}
                    \end{equation}
                    and by setting $S_{j}=R_{j}R_{j-1}...R_{0}$, we have
                    \begin{equation}
                        d_{j}=\frac
                        {
                            \begin{pmatrix}
                                1\\
                                0
                            \end{pmatrix}^{t}
                            S_{j}
                            \begin{pmatrix}
                                1\\
                                1
                            \end{pmatrix}
                        }
                        {
                            \begin{pmatrix}
                                0\\
                                1
                            \end{pmatrix}^{t}
                            S_{j}
                            \begin{pmatrix}
                                1\\
                                1
                            \end{pmatrix}
                        }
                    \end{equation} 

                    The algorithm is the following:
                    \begin{enumerate}
                        \item On each processor, matrix $R_{k}$ is created. $k$ is the number of rows
                        which have to be treated by the process.
                        \item On each processor, matrix $S_{j}$ is created.
                        \item Calculate local $d_{k}$.
                        \item Send local $d_{k}$ to next processor.
                        \item Each process calculate the local $l_{k}$
                        \item Distribute $d_{j}$ and $l_{j}$ values on all processors.
                        \item On each processor, a local forward and backward substitution are performed
                        to obtain the solution
                    \end{enumerate}

                    This algorithm is performed on each time step, and the found value is inserted in the 
                    result matrix. Previous time vector is used for actual time space.

                    \subsubsection{Crank-Nicholson}
                        We have :
                        \begin{equation}
                            \frac{T_{j}^{n+1} - T_{j}^n}{\Delta t} = D\frac{1}{2} (\frac{T_{j+1}^{n+1}- 2T_{j}^{n+1} + T_{j-1}^{n+1}}{\Delta x^2}+\frac{T_{j+1}^{n}- 2T_{j}^{n} + T_{j-1}^{n}}{\Delta x^2})
                        \end{equation}
                        The stencil for Crank trapezoidal scheme is the following:
                        \begin{figure}[H]
                            \includegraphics[width=4cm]{stencil_crank.png}
                            \centering
                            \caption{Stencil for Crank trapezoidal scheme}
                        \end{figure}
                        Written in a matrical way, the solution at space step $n+1$ is given by :
                        \begin{equation}
                            \begin{bmatrix}
                                1+C    & -C/2   & 0     & \cdots & 0 \\
                                -C/2   & 1+C    & -C/2   & \ddots & \vdots \\
                                0      & -C/2   & \ddots & \ddots & 0 \\
                                \vdots & \ddots & \ddots & 1+C   & -C/2\\
                                0      & \cdots & 0      & -C/2   & 1+C
                            \end{bmatrix}
                            \begin{bmatrix}
                                T_{1} \\
                                T_{2} \\
                                \vdots \\
                                T_{i} \\
                                \vdots \\
                                T_{ntime}
                            \end{bmatrix}_{n+1}
                            \label{eq:crank}
                        \end{equation}
                        \[
                            =
                            \begin{bmatrix}
                                1-C    &C/2    & 0      & \cdots & 0 \\
                                C/2     & 1-C   & C/2     & \ddots & \vdots \\
                                0      & C/2     & \ddots & \ddots & 0 \\
                                \vdots & \ddots & \ddots & 1-C   & C/2\\
                                0      & \cdots & 0      & C/2     & 1-C
                            \end{bmatrix}
                            \begin{bmatrix}
                                T_{1} + CT_{0}\\
                                T_{2} \\
                                \vdots \\
                                T_{i} \\
                                \vdots \\
                                T_{ntime} + CT_{f}
                            \end{bmatrix}_{n}
                        \]
                        with $$C = \frac{D\Delta t}{\Delta x ^2}  $$ 
                        The method and the algorithm used are the same as Laasonen scheme, except that the vector which is found with Thomas algorithm
                        is multiplied by the right-term matrix before the next time step.
    
                \subsection{Analytical solution}
                    The analytical solution of the problem considered will be used to compare our results and to calculate
                the errors. The result of the analytical solution for a time $t$ and a position $x$ is given by : 
                \begin{equation}
                    \label{eq:analytical}
                    T = T_{ext}+2(T_{int}-T_{ext})\sum_{m=1}^{m=\infty} e^{-D(m\pi / L)^{2}t} \frac{1-(-1)^m}{m\pi} sin(\frac{m\pi x}{L})
                \end{equation}

                \subsection{Speed-up calculation}
                    To investigate the performance of our programs, we used MPI functions :
                    \begin{itemize}
                        \item MPI\_Wtime : returns an elapsed time on the calling processor.\cite{mpi_doc}
                        \item MPI\_Wtick : returns the resolution of MPI\_Wtime.\cite{mpi_doc}
                    \end{itemize}
                    The actual speed-up is calculated as following:
                    \begin{equation}
                        speedup = finaltime - starttime
                    \end{equation}
                    where $finaltime$ is a double containing the value of MPI\_Wtime at the end of the program,
                    and $starttime$ is a double containing the value of MPI\_Wtime at the beginning of the program.
    
    
        % Results
        \newpage
        \section{Results}
            \subsection{Step size effect}

                We must note that due to the step sizes, first calculations could not be done in parallel.
                \begin{figure}[H]
                    \includegraphics[width=\textwidth]{step_size_1.png}
                    \caption{Results with differents serial schemes at $t = 0,5h$; $\Delta x=0,5$; $\Delta t=0,1$}
                \end{figure}

                We can see quite a big difference between the analytical solution and the other schemes. The errors had been calculated 
                to evaluate the accuracy of each method.

                \begin{table}[H]
                    \centering
                    \caption{Errors for different serial schemes at $t = 0,5h$; $\Delta x=0,5$; $\Delta t=0,1$}
                    \begin{tabular}{|c|c|c|l}
                    \cline{1-3}
                    FTCS     & Laasonen & Crank-Nicholson &  \\ \cline{1-3}
                    3,04\%   & 2,46\%   & 2,21\%          &  \\ \cline{1-3}
                    \end{tabular}
                    
                \end{table}

                We observe that for these step sizes, at t = 0,5h, Crank-Nicholson method is the 
                most accurate, and Forward Time Central Space is the less accurate. Let see the 
                differences when we decrease the step sizes.

                \begin{figure}[H]
                    \includegraphics[width=\textwidth]{step_size_2.png}
                    \caption{Results with differents parallelized schemes at $t = 0,5h$; $\Delta x=0,005$; $\Delta t=0,001$}
                \end{figure}

                With these time and space steps, the stability criterion for FCTS method was not 
                verified. The values are not displayed because of their lack of interest.
                We can see a very small difference between the analytical solution and the other schemes. The errors had been calculated 
                to evaluate the accuracy of each method.

                \begin{table}[H]
                    \centering
                    \caption{Errors for different parallelized schemes at $t = 0,5h$; $\Delta x=0,005$; $\Delta t=0,001$}
                    \begin{tabular}{|l|c|c|}
                        \hline
                                 & \multicolumn{1}{l|}{Laasonen} & \multicolumn{1}{l|}{Crank-Nicholson} \\ \hline
                        Serial   & 0,01\%                        & 0,04\%                               \\ \hline
                        Parallel & 0,10\%                        & 0,15\%                               \\ \hline
                    \end{tabular}
                \end{table}

                We observe that by reducing a lot the step size, accuracy of the results is very good. We also observe that serial
                results are quite the same as the analytical results. We will discuss this later.

            \subsection{Speed-up investigation}
                
            \begin{figure}[H]
                \includegraphics[width=\textwidth]{time_study.png}
                \caption{Speed-up results for each method}
            \end{figure}
            As we see in Fig. 6, the speed-up time increase as we increase the number of processors. The use of an external
            library is not quite usefull in our case.
            
        % Discussion
        \newpage
        \section{Discussion}
            \subsection{Analytical solution}
                First thing that should be considered is that the analytical solution should not be
                considered as the perfect exact solution, because of several factors.
                \\
                First, the analytical solution is calculated by the computer, so there may be some numerical error.
                \\
                Secondly, we have to consider the calculation of the sum inside equation \eqref{eq:analytical}.
                Indeed, as $m$ should be infinite, in theory, the biggest it is is the most accurate. However, increasing computational
                calculation may alterate the results. I choosed a compromise
                where values of the analytical solution at $t=0$ were as close to initial conditions as possible, with an $m$ not too big,
                to reduce run time and numerical error.
                \\
                
            \subsection{Step size study}
                With a big step size, we can see that serial results are quite accurate (less than 5\%). However,
                by reducing the time step and increasing the size of the grid, we observe better accuracy (less than 1\%)
                for serial results.\\
                So we can say that by increasing the size of the grid, we improve the results of the serial programs.\\

                However, we observe bigger errors for parallel programs than for serial programs, 
                for a small step size. \\
                That can be due to the fact that our problem is not complex enough, and putting it into parallel
                causes some more errors than in the serial program, which manages the problem well enough.
                We must consider the fact that our algorithms may not be the most efficients for the considered problem.


            \subsection{Performance study}
                We have observed that the parallelization of our serial programs was leading to a loss of runtime speed. But 
                the purpose of parallel programs is to reduce the speed-up time.\cite{hptc} One possible cause of this loss
                of speed could be, as seen before, the fact that our problem is not complex enough to be put in parallel. Another
                cause of this loss can also be the fact that our algorithms may not be the most efficients for the considered problem.
        % % Future work
        \newpage
        \section{Future work}
            Our future work would be based on three major axes : 
            \begin{itemize}
                \item{parallel algorithms improvement}
                \item{manage to use an external library}
                \item{study complexification}
            \end{itemize}
        % Conclusion
        \newpage
        \section{Conclusion}
            For physical problems, PDE can be resolved using discretisation schemes. A good way to increase the accuracy
            of the results is to increase the size of the study, by reducing the time and space steps. \\
            But 
            as the accuracy is increasing, so are the runtime and the computational costs. As these problems can be
            quite complex, to reduce computing time and to provide accurate results, parallelization can be used.\\
            It may not be interesting to perform a parallelization for every problem : indeed, in some cases, especially
            for small programs,
            the parallelization of a program may increase the runtime and the error. But for huge and complex
            programs, the parallelization allows to save runtime and ressources.\\
            
        % Aknowledgments
        % \newpage
        % \section{Aknowledgments}
    
        % References
        \newpage
        \section{References}
        \begin{thebibliography}{9}
            \bibitem{pde}
            Wazwaz, Abdul-Majid. \textit{Partial differential equations}. CRC Press, 2002.
            
            \bibitem{mpi_doc}
            Open MPI v2.0.4 documentation.
            \\\texttt{https://www.open-mpi.org/doc/v2.0/}
            
            \bibitem{ftcs}
            Luciano Rezzolla \textit{Numerical Methods for the Solution of Partial Differential Equations}, 2011
            
            \bibitem{hptc}
            Gottlieb, Allan; Almasi, George S. (1989). \textit{Highly parallel computing}. Redwood City, Calif.: Benjamin/Cummings.
            
            \bibitem{thomasalg1}
            George Karniadakis \textit{Parallel Scientific Computing in C++ and MPI}. 1994.

            \bibitem{thomasalg2}
            E.F. Van de Velde. \textit{Concurrent Scientific Computing}. 1994.

    
            
        \end{thebibliography}
        % Appendices
        \newpage
        \section{Appendices}
            \subsection{Code sample}
                \lstinputlisting[language=C++, caption=Analytical solution code]{../analytical.c}
                \lstinputlisting[language=C++, caption=FTCS code]{../ftcsv2.c}
                \lstinputlisting[language=C++, caption=Laasonen code]{../laasonen_MPI.c}
                \lstinputlisting[language=C++, caption=Crank-Nicholson code]{../crank_mpi.c}
        \end{document}